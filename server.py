from flask import Flask, request, jsonify
from urllib.parse import urlparse
import subprocess
import os
import traceback
from crawler import collect_links
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import numpy as np

app = Flask(__name__)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VECTOR_STORE_PATH = os.path.join(BASE_DIR, "vector_store", "doc_store.index")
DOCS_PATH = os.path.join(BASE_DIR, "vector_store", "doc_chunks.pkl")

# â–¶ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

@app.route("/health", methods=["GET"])
def health_check():
    return jsonify({"status": "ok"}), 200

@app.route("/start_data_ingestion", methods=["POST"])
def start_data_ingestion():
    try:
        data = request.get_json()
        site_url = data.get("site_url")

        if not site_url:
            return jsonify({"error": "site_url is required"}), 400

        parsed = urlparse(site_url)
        domain = parsed.netloc

        print(f"[INFO] ğŸ“¥ ìš”ì²­ëœ ì‚¬ì´íŠ¸: {site_url} (ë„ë©”ì¸: {domain})")
        print("[INFO] ğŸ” ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        collect_links(start_url=site_url, allowed_domains=[domain])

        print("[INFO] â¬‡ HTML ë‹¤ìš´ë¡œë“œ ì‹¤í–‰...")
        subprocess.run(["python", os.path.join(BASE_DIR, "html_downloader.py")], check=True)

        print("[INFO] ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤í–‰...")
        subprocess.run(["python", os.path.join(BASE_DIR, "parse_local_html.py")], check=True)

        print("[INFO] ğŸ” ë¬¸ì„œ ì„ë² ë”© ì‹¤í–‰...")
        subprocess.run(["python", os.path.join(BASE_DIR, "embed.py")], check=True)

        return jsonify({"status": "completed", "message": "ë¬¸ì„œ ìˆ˜ì§‘ ë° ì„ë² ë”© ì™„ë£Œ"}), 200

    except Exception as e:
        print(f"[ERROR] âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/ask", methods=["POST"])
def ask_question():
    try:
        data = request.get_json()
        query = data.get("query")

        if not query:
            return jsonify({"error": "query is required"}), 400

        if not os.path.exists(VECTOR_STORE_PATH) or not os.path.exists(DOCS_PATH):
            return jsonify({"error": "ë²¡í„° ì €ì¥ì†Œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 500

        print(f"[QUERY] ğŸ¤” ì‚¬ìš©ì ì§ˆë¬¸: {query}")
        query_vector = model.encode([query])
        query_vector = np.array(query_vector).astype("float32")

        # FAISS index ë¡œë“œ
        index = faiss.read_index(VECTOR_STORE_PATH)

        # ë¬¸ì„œ chunk ë¡œë“œ
        with open(DOCS_PATH, "rb") as f:
            doc_chunks = pickle.load(f)

        D, I = index.search(query_vector, k=5)
        matched_docs = [doc_chunks[i] for i in I[0]]

        return jsonify({"documents": matched_docs}), 200

    except Exception as e:
        print(f"[ERROR] âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
